{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b651c2a6",
   "metadata": {},
   "source": [
    "# 딥러닝 회귀 모델 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f0087f",
   "metadata": {},
   "source": [
    "```\n",
    "uv add pandas\n",
    "uv add scikit-learn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03c62d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cca8466d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "      <td>22.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "      <td>23.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0    0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1    0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2    0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3    0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4    0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "..       ...   ...    ...   ...    ...    ...   ...     ...  ...    ...   \n",
       "501  0.06263   0.0  11.93   0.0  0.573  6.593  69.1  2.4786  1.0  273.0   \n",
       "502  0.04527   0.0  11.93   0.0  0.573  6.120  76.7  2.2875  1.0  273.0   \n",
       "503  0.06076   0.0  11.93   0.0  0.573  6.976  91.0  2.1675  1.0  273.0   \n",
       "504  0.10959   0.0  11.93   0.0  0.573  6.794  89.3  2.3889  1.0  273.0   \n",
       "505  0.04741   0.0  11.93   0.0  0.573  6.030  80.8  2.5050  1.0  273.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  MEDV  \n",
       "0       15.3  396.90   4.98  24.0  \n",
       "1       17.8  396.90   9.14  21.6  \n",
       "2       17.8  392.83   4.03  34.7  \n",
       "3       18.7  394.63   2.94  33.4  \n",
       "4       18.7  396.90   5.33  36.2  \n",
       "..       ...     ...    ...   ...  \n",
       "501     21.0  391.99   9.67  22.4  \n",
       "502     21.0  396.90   9.08  20.6  \n",
       "503     21.0  396.90   5.64  23.9  \n",
       "504     21.0  393.45   6.48  22.0  \n",
       "505     21.0  396.90   7.88  11.9  \n",
       "\n",
       "[506 rows x 14 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/boston.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "456632ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 14 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   CRIM     506 non-null    float64\n",
      " 1   ZN       506 non-null    float64\n",
      " 2   INDUS    506 non-null    float64\n",
      " 3   CHAS     506 non-null    float64\n",
      " 4   NOX      506 non-null    float64\n",
      " 5   RM       506 non-null    float64\n",
      " 6   AGE      506 non-null    float64\n",
      " 7   DIS      506 non-null    float64\n",
      " 8   RAD      506 non-null    float64\n",
      " 9   TAX      506 non-null    float64\n",
      " 10  PTRATIO  506 non-null    float64\n",
      " 11  B        506 non-null    float64\n",
      " 12  LSTAT    506 non-null    float64\n",
      " 13  MEDV     506 non-null    float64\n",
      "dtypes: float64(14)\n",
      "memory usage: 55.5 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46c73ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "X_train = df.iloc[:,:13].values\n",
    "# X_train = torch.tensor(X_train)       # tensor 로 변경 1\n",
    "X_train = torch.FloatTensor(X_train)    # tensor 로 변경 2\n",
    "\n",
    "y_train = df.iloc[:, -1].values\n",
    "y_train = torch.FloatTensor(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6130042f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=13, out_features=100, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=100, out_features=50, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=50, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim.adam import Adam\n",
    "\n",
    "# Adam : 최적화 함수(optimize func)\n",
    "# input-layer : 13\n",
    "# hidden-layer : 맘대로 (여기선 100으로 설정)\n",
    "# output-layer : 1 (정답은 1개)\n",
    "# 13 -> 100 -> 50 -> 1\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(13, 100),       # input으로 들어가는 피처의 개수(13), 나온는 개수(100)\n",
    "    nn.ReLU(),                # 활성화 함수\n",
    "    nn.Linear(100, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 1)\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b20659c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 14)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7e337e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name : 0.weight\n",
      "\n",
      "Shape : torch.Size([100, 13])\n",
      "\n",
      "Values : Parameter containing:\n",
      "tensor([[ 0.1839,  0.0966, -0.1421,  ..., -0.0711,  0.0840, -0.0952],\n",
      "        [-0.2041, -0.1420,  0.0316,  ..., -0.1236, -0.1291,  0.2635],\n",
      "        [ 0.0949, -0.1363, -0.1978,  ...,  0.1306, -0.1584,  0.1736],\n",
      "        ...,\n",
      "        [ 0.1415,  0.2434,  0.1328,  ...,  0.1908, -0.2196,  0.2000],\n",
      "        [-0.0615,  0.2703, -0.0400,  ..., -0.1039, -0.1563, -0.1541],\n",
      "        [-0.1505, -0.0848,  0.1190,  ..., -0.0991, -0.1231,  0.0595]],\n",
      "       requires_grad=True)\n",
      "\n",
      "Name : 0.bias\n",
      "\n",
      "Shape : torch.Size([100])\n",
      "\n",
      "Values : Parameter containing:\n",
      "tensor([-0.2164, -0.1036, -0.0083,  0.2668,  0.2288, -0.0398,  0.0396,  0.0951,\n",
      "        -0.0949,  0.1113,  0.0906, -0.0782, -0.0841,  0.2657,  0.0913, -0.1717,\n",
      "        -0.2597,  0.1732,  0.0990,  0.2464,  0.0930, -0.0235,  0.0502, -0.0478,\n",
      "        -0.1418, -0.1242,  0.0311, -0.2054, -0.1552,  0.0615, -0.1356,  0.1340,\n",
      "        -0.0784, -0.0698,  0.2329, -0.1568, -0.0909,  0.2265, -0.0190, -0.2286,\n",
      "        -0.1554,  0.1440, -0.0432,  0.2512, -0.2528, -0.0290,  0.0457,  0.1405,\n",
      "        -0.1347,  0.0326, -0.1042, -0.1074,  0.1981, -0.1483,  0.2719,  0.1488,\n",
      "        -0.1580,  0.1202,  0.0012,  0.2070,  0.1108, -0.0751,  0.1188,  0.0176,\n",
      "        -0.1332,  0.1991,  0.1885, -0.1940,  0.2049,  0.0043,  0.1722,  0.1168,\n",
      "         0.0021, -0.0730,  0.2051,  0.2584,  0.0135, -0.1826, -0.0016,  0.2243,\n",
      "        -0.0040,  0.2683, -0.0765,  0.1102,  0.1738,  0.2317,  0.0943, -0.1122,\n",
      "        -0.1458, -0.0183, -0.0437, -0.0945,  0.2755,  0.2476, -0.2168, -0.1552,\n",
      "        -0.1667, -0.1689,  0.1036, -0.0227], requires_grad=True)\n",
      "\n",
      "Name : 2.weight\n",
      "\n",
      "Shape : torch.Size([50, 100])\n",
      "\n",
      "Values : Parameter containing:\n",
      "tensor([[-0.0239, -0.0307,  0.0671,  ..., -0.0736,  0.0015, -0.0824],\n",
      "        [-0.0649, -0.0333,  0.0282,  ...,  0.0551, -0.0678, -0.0466],\n",
      "        [ 0.0116, -0.0349,  0.0442,  ..., -0.0117, -0.0604,  0.0714],\n",
      "        ...,\n",
      "        [-0.0473, -0.0831,  0.0664,  ..., -0.0886,  0.0129, -0.0039],\n",
      "        [-0.0961, -0.0815,  0.0759,  ..., -0.0533, -0.0300,  0.0229],\n",
      "        [ 0.0910, -0.0726,  0.0089,  ..., -0.0589, -0.0253,  0.0191]],\n",
      "       requires_grad=True)\n",
      "\n",
      "Name : 2.bias\n",
      "\n",
      "Shape : torch.Size([50])\n",
      "\n",
      "Values : Parameter containing:\n",
      "tensor([ 0.0247, -0.0883, -0.0644,  0.0403,  0.0025, -0.0042,  0.0483, -0.0619,\n",
      "        -0.0303, -0.0817, -0.0897,  0.0010, -0.0769, -0.0475, -0.0318, -0.0531,\n",
      "         0.0632,  0.0899, -0.0290,  0.0014,  0.0678, -0.0491,  0.0455, -0.0823,\n",
      "        -0.0388,  0.0696,  0.0683, -0.0177,  0.0675,  0.0751, -0.0439, -0.0915,\n",
      "        -0.0288,  0.0962, -0.0650, -0.0219,  0.0893,  0.0166,  0.0544,  0.0251,\n",
      "         0.0555, -0.0773, -0.0461, -0.0277,  0.0173, -0.0271,  0.0093, -0.0765,\n",
      "        -0.0395,  0.0564], requires_grad=True)\n",
      "\n",
      "Name : 4.weight\n",
      "\n",
      "Shape : torch.Size([1, 50])\n",
      "\n",
      "Values : Parameter containing:\n",
      "tensor([[-0.1309,  0.0171,  0.0591,  0.0569,  0.0273, -0.0614,  0.0766, -0.0800,\n",
      "          0.0055, -0.0869,  0.1118,  0.1086, -0.0671, -0.1225, -0.0687, -0.1336,\n",
      "         -0.0575, -0.0079,  0.1061, -0.1126, -0.0354,  0.0522,  0.0645,  0.0278,\n",
      "         -0.0368,  0.0409,  0.0106,  0.1163, -0.0515,  0.0805,  0.1331, -0.0989,\n",
      "          0.1186,  0.1356,  0.0615, -0.0288, -0.1242,  0.0612, -0.0803, -0.0287,\n",
      "          0.0976, -0.0974,  0.0249, -0.1153, -0.1193, -0.0816,  0.0115, -0.0360,\n",
      "          0.0778, -0.0871]], requires_grad=True)\n",
      "\n",
      "Name : 4.bias\n",
      "\n",
      "Shape : torch.Size([1])\n",
      "\n",
      "Values : Parameter containing:\n",
      "tensor([-0.0138], requires_grad=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters() :\n",
    "    print(f\"Name : {name}\\n\")\n",
    "    print(f\"Shape : {param.shape}\\n\")\n",
    "    print(f\"Values : {param}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c5a106c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e57d125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU로 데이터 올리기\n",
    "X_train = X_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1eb333",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\deep_learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:616: UserWarning: Using a target size (torch.Size([506])) that is different to the input size (torch.Size([506, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 780.7772827148438\n",
      "loss : 385.27532958984375\n",
      "loss : 172.17556762695312\n",
      "loss : 107.21012878417969\n",
      "loss : 133.53103637695312\n",
      "loss : 191.47982788085938\n",
      "loss : 231.47576904296875\n",
      "loss : 236.1537628173828\n",
      "loss : 213.1333770751953\n",
      "loss : 176.9108428955078\n",
      "loss : 140.39254760742188\n",
      "loss : 112.73358154296875\n",
      "loss : 98.5892105102539\n",
      "loss : 96.86318969726562\n",
      "loss : 104.08589935302734\n",
      "loss : 115.15142059326172\n",
      "loss : 124.96925354003906\n",
      "loss : 130.0097198486328\n",
      "loss : 128.96817016601562\n",
      "loss : 122.57808685302734\n",
      "loss : 113.01004791259766\n",
      "loss : 103.04911804199219\n",
      "loss : 95.1307601928711\n",
      "loss : 90.80170440673828\n",
      "loss : 90.56224822998047\n",
      "loss : 93.57649993896484\n",
      "loss : 97.5818862915039\n",
      "loss : 100.45639038085938\n",
      "loss : 101.07807159423828\n",
      "loss : 99.40072631835938\n",
      "loss : 96.33110809326172\n",
      "loss : 92.98711395263672\n",
      "loss : 90.20895385742188\n",
      "loss : 88.59783935546875\n",
      "loss : 88.2713851928711\n",
      "loss : 88.94107055664062\n",
      "loss : 90.06566619873047\n",
      "loss : 91.0745849609375\n",
      "loss : 91.55782318115234\n",
      "loss : 91.3597640991211\n",
      "loss : 90.58751678466797\n",
      "loss : 89.53440856933594\n",
      "loss : 88.55477142333984\n",
      "loss : 87.93048095703125\n",
      "loss : 87.77616119384766\n",
      "loss : 88.01575469970703\n",
      "loss : 88.4422378540039\n",
      "loss : 88.81317138671875\n",
      "loss : 88.94901275634766\n",
      "loss : 88.79317474365234\n",
      "loss : 88.4167251586914\n",
      "loss : 87.96832275390625\n",
      "loss : 87.60136413574219\n",
      "loss : 87.41447448730469\n",
      "loss : 87.41978454589844\n",
      "loss : 87.5538101196289\n",
      "loss : 87.71588897705078\n",
      "loss : 87.81398010253906\n",
      "loss : 87.79923248291016\n",
      "loss : 87.67860412597656\n",
      "loss : 87.50297546386719\n",
      "loss : 87.33837890625\n",
      "loss : 87.23571014404297\n",
      "loss : 87.21161651611328\n",
      "loss : 87.24653625488281\n",
      "loss : 87.29867553710938\n",
      "loss : 87.32495880126953\n",
      "loss : 87.30071258544922\n",
      "loss : 87.22857666015625\n",
      "loss : 87.13299560546875\n",
      "loss : 87.04573059082031\n",
      "loss : 86.98910522460938\n",
      "loss : 86.9682846069336\n",
      "loss : 86.97181701660156\n",
      "loss : 86.9776840209961\n",
      "loss : 86.96813201904297\n",
      "loss : 86.93671417236328\n",
      "loss : 86.88873291015625\n",
      "loss : 86.837646484375\n",
      "loss : 86.79695129394531\n",
      "loss : 86.77159881591797\n",
      "loss : 86.75919342041016\n",
      "loss : 86.74844360351562\n",
      "loss : 86.73043060302734\n",
      "loss : 86.70064544677734\n",
      "loss : 86.66162109375\n",
      "loss : 86.61969757080078\n",
      "loss : 86.5831527709961\n",
      "loss : 86.55613708496094\n",
      "loss : 86.53994750976562\n",
      "loss : 86.52804565429688\n",
      "loss : 86.51461791992188\n",
      "loss : 86.4991226196289\n",
      "loss : 86.48190307617188\n",
      "loss : 86.46501922607422\n",
      "loss : 86.45094299316406\n",
      "loss : 86.43943786621094\n",
      "loss : 86.42866516113281\n",
      "loss : 86.41667938232422\n",
      "loss : 86.40274810791016\n",
      "loss : 86.3874740600586\n",
      "loss : 86.37224578857422\n",
      "loss : 86.35823822021484\n",
      "loss : 86.3456802368164\n",
      "loss : 86.33369445800781\n",
      "loss : 86.32120513916016\n",
      "loss : 86.30775451660156\n",
      "loss : 86.29373931884766\n",
      "loss : 86.28003692626953\n",
      "loss : 86.26736450195312\n",
      "loss : 86.2557601928711\n",
      "loss : 86.24462127685547\n",
      "loss : 86.23339080810547\n",
      "loss : 86.22187805175781\n",
      "loss : 86.21034240722656\n",
      "loss : 86.19927215576172\n",
      "loss : 86.18885803222656\n",
      "loss : 86.17884826660156\n",
      "loss : 86.16891479492188\n",
      "loss : 86.15872955322266\n",
      "loss : 86.14833068847656\n",
      "loss : 86.13788604736328\n",
      "loss : 86.127685546875\n",
      "loss : 86.1178207397461\n",
      "loss : 86.10804748535156\n",
      "loss : 86.09822845458984\n",
      "loss : 86.08834838867188\n",
      "loss : 86.07847595214844\n",
      "loss : 86.06880187988281\n",
      "loss : 86.05938720703125\n",
      "loss : 86.05004119873047\n",
      "loss : 86.04063415527344\n",
      "loss : 86.0311508178711\n",
      "loss : 86.02149200439453\n",
      "loss : 86.01164245605469\n",
      "loss : 86.001953125\n",
      "loss : 85.99256896972656\n",
      "loss : 85.98320007324219\n",
      "loss : 85.97380828857422\n",
      "loss : 85.96438598632812\n",
      "loss : 85.95511627197266\n",
      "loss : 85.94602966308594\n",
      "loss : 85.93711853027344\n",
      "loss : 85.92830657958984\n",
      "loss : 85.91961669921875\n",
      "loss : 85.91090393066406\n",
      "loss : 85.901611328125\n",
      "loss : 85.89241027832031\n",
      "loss : 85.88327026367188\n",
      "loss : 85.87413787841797\n",
      "loss : 85.86512756347656\n",
      "loss : 85.85623931884766\n",
      "loss : 85.84696197509766\n",
      "loss : 85.83721160888672\n",
      "loss : 85.82734680175781\n",
      "loss : 85.81742095947266\n",
      "loss : 85.80741882324219\n",
      "loss : 85.79734802246094\n",
      "loss : 85.78726196289062\n",
      "loss : 85.77719116210938\n",
      "loss : 85.76712799072266\n",
      "loss : 85.75704193115234\n",
      "loss : 85.74697875976562\n",
      "loss : 85.73688507080078\n",
      "loss : 85.7268295288086\n",
      "loss : 85.71683502197266\n",
      "loss : 85.70691680908203\n",
      "loss : 85.69707489013672\n",
      "loss : 85.68729400634766\n",
      "loss : 85.67756652832031\n",
      "loss : 85.66792297363281\n",
      "loss : 85.65834045410156\n",
      "loss : 85.64884948730469\n",
      "loss : 85.63945770263672\n",
      "loss : 85.63018035888672\n",
      "loss : 85.6210708618164\n",
      "loss : 85.61209106445312\n",
      "loss : 85.6032485961914\n",
      "loss : 85.594482421875\n",
      "loss : 85.5857925415039\n",
      "loss : 85.57726287841797\n",
      "loss : 85.56875610351562\n",
      "loss : 85.56029510498047\n",
      "loss : 85.55197143554688\n",
      "loss : 85.54373931884766\n",
      "loss : 85.53570556640625\n",
      "loss : 85.52784729003906\n",
      "loss : 85.52017974853516\n",
      "loss : 85.51270294189453\n",
      "loss : 85.50535583496094\n",
      "loss : 85.49815368652344\n",
      "loss : 85.4911117553711\n",
      "loss : 85.4842758178711\n",
      "loss : 85.4776382446289\n",
      "loss : 85.47106170654297\n",
      "loss : 85.46461486816406\n",
      "loss : 85.4582290649414\n",
      "loss : 85.45189666748047\n",
      "loss : 85.44561004638672\n",
      "loss : 85.43940734863281\n"
     ]
    }
   ],
   "source": [
    "optim = Adam(model.parameters(), lr=0.001)  # 최적화함수\n",
    "epochs = 200                                # 몇 번 학습 시킬지\n",
    "criterion = nn.MSELoss()                    # 손실함수\n",
    "\n",
    "for epoch in range(epochs) :\n",
    "    optim.zero_grad()                   # 최적화함수 기울기 초기화\n",
    "    y_pred = model(X_train)             # 예측값\n",
    "    loss = criterion(y_pred, y_train)   # 손실함수\n",
    "    loss.backward()                     # 역전파\n",
    "    optim.step()                        # 최적화 (가중치 업데이트)\n",
    "\n",
    "    print(f\"loss : {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52679629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[ 0.1798,  0.1026, -0.1365,  ..., -0.0652,  0.0868, -0.0875],\n",
       "                      [-0.1679, -0.1420,  0.0643,  ..., -0.0956, -0.1156,  0.2872],\n",
       "                      [ 0.0949, -0.1363, -0.1978,  ...,  0.1306, -0.1584,  0.1736],\n",
       "                      ...,\n",
       "                      [ 0.1409,  0.2375,  0.1358,  ...,  0.1939, -0.2203,  0.2046],\n",
       "                      [-0.0615,  0.2703, -0.0400,  ..., -0.1039, -0.1563, -0.1541],\n",
       "                      [-0.1479, -0.0780,  0.1241,  ..., -0.0930, -0.1179,  0.0659]],\n",
       "                     device='cuda:0')),\n",
       "             ('0.bias',\n",
       "              tensor([-0.2078, -0.0642, -0.0083,  0.2547,  0.2184, -0.0398,  0.0267,  0.0970,\n",
       "                      -0.1098,  0.1015,  0.0906, -0.0711, -0.0841,  0.2556,  0.1020, -0.1785,\n",
       "                      -0.2597,  0.1640,  0.0841,  0.2464,  0.0965, -0.0241,  0.0502, -0.0478,\n",
       "                      -0.1448, -0.0993,  0.0311, -0.2363, -0.1645,  0.0834, -0.1356,  0.1261,\n",
       "                      -0.0860, -0.0795,  0.2329, -0.1568, -0.0909,  0.2169, -0.0210, -0.2286,\n",
       "                      -0.1500,  0.1385, -0.0562,  0.3344, -0.2528, -0.0369,  0.0329,  0.2878,\n",
       "                      -0.1287,  0.0381, -0.1124, -0.1074,  0.2133, -0.1483,  0.2702,  0.1413,\n",
       "                      -0.1562,  0.1325,  0.0012,  0.2136,  0.1203, -0.0751,  0.1409,  0.0176,\n",
       "                      -0.1416,  0.1949,  0.1808, -0.2046,  0.2090,  0.0117,  0.1623,  0.1168,\n",
       "                      -0.0062, -0.0730,  0.2496,  0.2584,  0.0460, -0.1738, -0.0111,  0.2243,\n",
       "                      -0.0040,  0.2753, -0.0702,  0.1781,  0.1646,  0.2317,  0.0842, -0.1203,\n",
       "                      -0.1458, -0.0183, -0.0359, -0.0875,  0.2576,  0.2476, -0.2190, -0.1519,\n",
       "                      -0.1727, -0.1637,  0.1036, -0.0161], device='cuda:0')),\n",
       "             ('2.weight',\n",
       "              tensor([[-0.0325, -0.0382,  0.0671,  ..., -0.0818,  0.0015, -0.0909],\n",
       "                      [-0.0649, -0.0333,  0.0282,  ...,  0.0551, -0.0678, -0.0466],\n",
       "                      [ 0.0143, -0.0183,  0.0442,  ..., -0.0105, -0.0604,  0.0720],\n",
       "                      ...,\n",
       "                      [-0.0473, -0.0831,  0.0664,  ..., -0.0886,  0.0129, -0.0039],\n",
       "                      [-0.0936, -0.0651,  0.0759,  ..., -0.0521, -0.0300,  0.0235],\n",
       "                      [ 0.0876, -0.0894,  0.0089,  ..., -0.0608, -0.0253,  0.0178]],\n",
       "                     device='cuda:0')),\n",
       "             ('2.bias',\n",
       "              tensor([ 0.0159, -0.0883, -0.0558,  0.0487,  0.0097, -0.0136,  0.0568, -0.0704,\n",
       "                      -0.0248, -0.0885, -0.0811,  0.0010, -0.0860, -0.0563, -0.0389, -0.0531,\n",
       "                       0.0632,  0.0825, -0.0203, -0.0066,  0.0597, -0.0430,  0.0428, -0.0829,\n",
       "                      -0.0472,  0.0751,  0.0683, -0.0177,  0.0583,  0.0751, -0.0367, -0.1017,\n",
       "                      -0.0288,  0.0962, -0.0650, -0.0294,  0.0833,  0.0114,  0.0544,  0.0251,\n",
       "                       0.0641, -0.0854, -0.0384, -0.0344,  0.0173, -0.0359,  0.0093, -0.0765,\n",
       "                      -0.0311,  0.0473], device='cuda:0')),\n",
       "             ('4.weight',\n",
       "              tensor([[-1.2301e-01,  1.7051e-02,  6.0992e-02,  5.7004e-02,  2.5761e-02,\n",
       "                       -5.2712e-02,  7.7901e-02, -7.0148e-02,  7.7502e-03, -8.0395e-02,\n",
       "                        1.1040e-01,  1.0857e-01, -6.2913e-02, -1.1336e-01, -6.1072e-02,\n",
       "                       -1.3358e-01, -5.7473e-02, -5.8270e-05,  1.0754e-01, -1.0527e-01,\n",
       "                       -2.7516e-02,  5.6110e-02,  6.1319e-02,  2.2476e-02, -3.0729e-02,\n",
       "                        4.0716e-02,  1.0611e-02,  1.1626e-01, -4.8065e-02,  8.0494e-02,\n",
       "                        1.3287e-01, -8.9099e-02,  1.1860e-01,  1.3556e-01,  6.1498e-02,\n",
       "                       -2.0425e-02, -1.1817e-01,  5.5208e-02, -8.0258e-02, -2.8727e-02,\n",
       "                        9.6638e-02, -9.0264e-02,  2.4535e-02, -1.0888e-01, -1.1932e-01,\n",
       "                       -7.3986e-02,  1.1496e-02, -3.5992e-02,  7.5998e-02, -8.2794e-02]],\n",
       "                     device='cuda:0')),\n",
       "             ('4.bias', tensor([-0.0049], device='cuda:0'))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습된 가중치의 값\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1fd5bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\deep_learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:616: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([100, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 267.0063781738281\n",
      "loss : 38.133941650390625\n",
      "loss : 31.8334903717041\n",
      "loss : 50.75652313232422\n",
      "loss : 40.99457550048828\n",
      "loss : 34.81953048706055\n",
      "loss : 38.86934280395508\n",
      "loss : 47.053916931152344\n",
      "loss : 38.989540100097656\n",
      "loss : 39.30032730102539\n",
      "loss : 43.941524505615234\n",
      "loss : 39.99186325073242\n",
      "loss : 38.092864990234375\n",
      "loss : 40.12855529785156\n",
      "loss : 39.493988037109375\n",
      "loss : 38.62429428100586\n",
      "loss : 39.74179458618164\n",
      "loss : 39.51298141479492\n",
      "loss : 38.89961242675781\n",
      "loss : 39.206912994384766\n",
      "loss : 38.97393035888672\n",
      "loss : 38.68368148803711\n",
      "loss : 38.84917068481445\n",
      "loss : 38.77202224731445\n",
      "loss : 38.679405212402344\n",
      "loss : 38.74753952026367\n",
      "loss : 38.57944107055664\n",
      "loss : 38.445919036865234\n",
      "loss : 38.452938079833984\n",
      "loss : 38.39507293701172\n",
      "loss : 38.36161804199219\n",
      "loss : 38.34721374511719\n",
      "loss : 38.288414001464844\n",
      "loss : 38.26985549926758\n",
      "loss : 38.253868103027344\n",
      "loss : 38.21651077270508\n",
      "loss : 38.21010971069336\n",
      "loss : 38.19007873535156\n",
      "loss : 38.169715881347656\n",
      "loss : 38.177635192871094\n",
      "loss : 38.173675537109375\n",
      "loss : 38.17118453979492\n",
      "loss : 38.161441802978516\n",
      "loss : 38.12482452392578\n",
      "loss : 38.139366149902344\n",
      "loss : 38.12438201904297\n",
      "loss : 38.10211944580078\n",
      "loss : 38.078147888183594\n",
      "loss : 38.08464431762695\n",
      "loss : 38.0656852722168\n",
      "loss : 38.02855682373047\n",
      "loss : 38.04684066772461\n",
      "loss : 37.995994567871094\n",
      "loss : 37.99440383911133\n",
      "loss : 38.01011657714844\n",
      "loss : 37.949462890625\n",
      "loss : 38.00836944580078\n",
      "loss : 37.96672821044922\n",
      "loss : 37.93877410888672\n",
      "loss : 37.9693603515625\n",
      "loss : 37.8994255065918\n",
      "loss : 37.943748474121094\n",
      "loss : 37.97288131713867\n",
      "loss : 37.94384002685547\n",
      "loss : 37.979515075683594\n",
      "loss : 37.92596435546875\n",
      "loss : 37.969810485839844\n",
      "loss : 37.93867874145508\n",
      "loss : 37.932891845703125\n",
      "loss : 37.961700439453125\n",
      "loss : 37.90155029296875\n",
      "loss : 37.96159744262695\n",
      "loss : 37.941566467285156\n",
      "loss : 37.93899917602539\n",
      "loss : 37.93971252441406\n",
      "loss : 37.9432258605957\n",
      "loss : 37.904823303222656\n",
      "loss : 37.95199966430664\n",
      "loss : 37.88751220703125\n",
      "loss : 37.91340255737305\n",
      "loss : 37.93038558959961\n",
      "loss : 37.89298629760742\n",
      "loss : 37.9138069152832\n",
      "loss : 37.89411926269531\n",
      "loss : 37.8690071105957\n",
      "loss : 37.932655334472656\n",
      "loss : 37.83054733276367\n",
      "loss : 37.90385055541992\n",
      "loss : 37.855587005615234\n",
      "loss : 37.90950012207031\n",
      "loss : 37.84048080444336\n",
      "loss : 37.8904914855957\n",
      "loss : 37.869468688964844\n",
      "loss : 37.83427810668945\n",
      "loss : 37.89595413208008\n",
      "loss : 37.81815719604492\n",
      "loss : 37.870967864990234\n",
      "loss : 37.833106994628906\n",
      "loss : 37.85955047607422\n",
      "loss : 37.84728240966797\n",
      "loss : 37.8277473449707\n",
      "loss : 37.874813079833984\n",
      "loss : 37.79803466796875\n",
      "loss : 37.85490036010742\n",
      "loss : 37.847354888916016\n",
      "loss : 37.785797119140625\n",
      "loss : 37.83201217651367\n",
      "loss : 37.79966735839844\n",
      "loss : 37.79340362548828\n",
      "loss : 37.77568817138672\n",
      "loss : 37.79938507080078\n",
      "loss : 37.7598876953125\n",
      "loss : 37.785430908203125\n",
      "loss : 37.740970611572266\n",
      "loss : 37.77341842651367\n",
      "loss : 37.75190353393555\n",
      "loss : 37.757537841796875\n",
      "loss : 37.787296295166016\n",
      "loss : 37.72782897949219\n",
      "loss : 37.7701530456543\n",
      "loss : 37.74113845825195\n",
      "loss : 37.76246643066406\n",
      "loss : 37.71324157714844\n",
      "loss : 37.725440979003906\n",
      "loss : 37.70853805541992\n",
      "loss : 37.71031951904297\n",
      "loss : 37.72866439819336\n",
      "loss : 37.68886947631836\n",
      "loss : 37.681861877441406\n",
      "loss : 37.65542984008789\n",
      "loss : 37.68809127807617\n",
      "loss : 37.655731201171875\n",
      "loss : 37.701995849609375\n",
      "loss : 37.69467544555664\n",
      "loss : 37.66016387939453\n",
      "loss : 37.69871139526367\n",
      "loss : 37.63916778564453\n",
      "loss : 37.6658821105957\n",
      "loss : 37.64790344238281\n",
      "loss : 37.63273239135742\n",
      "loss : 37.625301361083984\n",
      "loss : 37.611392974853516\n",
      "loss : 37.642486572265625\n",
      "loss : 37.583221435546875\n",
      "loss : 37.62697219848633\n",
      "loss : 37.57954406738281\n",
      "loss : 37.571292877197266\n",
      "loss : 37.5805549621582\n",
      "loss : 37.53797912597656\n",
      "loss : 37.575069427490234\n",
      "loss : 37.529869079589844\n",
      "loss : 37.533267974853516\n",
      "loss : 37.51719284057617\n",
      "loss : 37.560157775878906\n",
      "loss : 37.47433090209961\n",
      "loss : 37.56196594238281\n",
      "loss : 37.47126388549805\n",
      "loss : 37.556724548339844\n",
      "loss : 37.47084426879883\n",
      "loss : 37.53162384033203\n",
      "loss : 37.44595718383789\n",
      "loss : 37.52565002441406\n",
      "loss : 37.46879196166992\n",
      "loss : 37.43902587890625\n",
      "loss : 37.49519348144531\n",
      "loss : 37.44524383544922\n",
      "loss : 37.47795486450195\n",
      "loss : 37.43262481689453\n",
      "loss : 37.44029235839844\n",
      "loss : 37.42776870727539\n",
      "loss : 37.400142669677734\n",
      "loss : 37.425254821777344\n",
      "loss : 37.38601303100586\n",
      "loss : 37.39583206176758\n",
      "loss : 37.34347915649414\n",
      "loss : 37.39613342285156\n",
      "loss : 37.350643157958984\n",
      "loss : 37.36052703857422\n",
      "loss : 37.345314025878906\n",
      "loss : 37.3133430480957\n",
      "loss : 37.33290100097656\n",
      "loss : 37.30545425415039\n",
      "loss : 37.335506439208984\n",
      "loss : 37.28056335449219\n",
      "loss : 37.30545425415039\n",
      "loss : 37.272743225097656\n",
      "loss : 37.29949951171875\n",
      "loss : 37.25101089477539\n",
      "loss : 37.24831008911133\n",
      "loss : 37.284542083740234\n",
      "loss : 37.19389343261719\n",
      "loss : 37.30294418334961\n",
      "loss : 37.164947509765625\n",
      "loss : 37.27423095703125\n",
      "loss : 37.204017639160156\n",
      "loss : 37.2424430847168\n",
      "loss : 37.19330596923828\n",
      "loss : 37.19557571411133\n",
      "loss : 37.21019744873047\n",
      "loss : 37.17117691040039\n"
     ]
    }
   ],
   "source": [
    "# 배치 사용\n",
    "from torch.optim.adam import Adam\n",
    "\n",
    "optim = Adam(model.parameters(), lr=0.001)\n",
    "epochs = 200\n",
    "batch_size = 100\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch in range(epochs) :\n",
    "    for temp in range(len(X_train)//batch_size) :\n",
    "        s = temp * batch_size   # 0 * 100\n",
    "        e = s + batch_size      # 100\n",
    "\n",
    "        X = X_train[s:e]\n",
    "        y = y_train[s:e]\n",
    "\n",
    "        optim.zero_grad()\n",
    "        y_pred = model(X)\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "    print(f\"loss : {loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
