{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c52d2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b60679",
   "metadata": {},
   "source": [
    "```\n",
    "uv pip install torch torchvision --index-url https://download.pytorch.org/whl/cu126\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79ad86c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6168ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 전처리해주면 test도 같은 전처리 해줘야함\n",
    "\n",
    "data_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),              # tensor 변환\n",
    "        transforms.Resize(32),              # 이미지 크기변경\n",
    "        transforms.Normalize((0.5), (1.0))  # 정규화 (평균, 표준편차)\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_data = datasets.MNIST(root=\"./\", train=True, download=True, transform=data_transform)\n",
    "test_data = datasets.MNIST(root=\"./\", train=False, download=True, transform=data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f798861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.data.shape\n",
    "# 안바뀜 -> dataLoader를 통해 가져와야 바뀜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e48d6d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f726554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000],\n",
       "          [-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000],\n",
       "          [-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000],\n",
       "          ...,\n",
       "          [-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000],\n",
       "          [-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000],\n",
       "          [-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000]]],\n",
       "\n",
       "\n",
       "        [[[-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000],\n",
       "          [-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000],\n",
       "          [-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000],\n",
       "          ...,\n",
       "          [-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000],\n",
       "          [-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000],\n",
       "          [-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000]]],\n",
       "\n",
       "\n",
       "        [[[-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000],\n",
       "          [-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000],\n",
       "          [-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000],\n",
       "          ...,\n",
       "          [-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000],\n",
       "          [-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000],\n",
       "          [-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000],\n",
       "          [-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000],\n",
       "          [-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000],\n",
       "          ...,\n",
       "          [-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000],\n",
       "          [-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000],\n",
       "          [-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000]]],\n",
       "\n",
       "\n",
       "        [[[-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000],\n",
       "          [-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000],\n",
       "          [-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000],\n",
       "          ...,\n",
       "          [-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000],\n",
       "          [-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000],\n",
       "          [-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000]]],\n",
       "\n",
       "\n",
       "        [[[-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000],\n",
       "          [-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000],\n",
       "          [-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000],\n",
       "          ...,\n",
       "          [-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000],\n",
       "          [-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000],\n",
       "          [-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000]]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))[0]\n",
    "# 정규화되었기 때문에 값이 바뀌었다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "746603cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "data, label = next(iter(train_loader))\n",
    "print(data.shape)\n",
    "# [60000, 28, 28] -> [32, 1, 32, 32] 바뀜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cd0c4f",
   "metadata": {},
   "source": [
    "- Lenet5 : http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6b97f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지의 사이즈에 따라 fc1 의 in_features, fc2의 out_features 만 바꿔서 사용하면 된다\n",
    "# 보통은 논문 그대로 쓰고, full connection 만 바꿔줌\n",
    "\n",
    "class Lenet(nn.Module) :     # class 이름이 곧 모델명(Lenet)\n",
    "    def __init__(self) :\n",
    "        super(Lenet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5, stride=1)\n",
    "        self.fc1 = nn.Linear(in_features=120, out_features=84)\n",
    "        self.fc2 = nn.Linear(in_features=84, out_features=10)\n",
    "\n",
    "    def forward(self, x) :\n",
    "        x = self.conv1(x)\n",
    "        x = F.tanh(x)               # 6, 28, 28 (활성화함수)\n",
    "        x = F.max_pool2d(x, 2, 2)   # 6, 14, 14 (2X2 max pooling)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.tanh(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = F.tanh(x)\n",
    "        x = x.view(-1, 120)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = F.tanh(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = F.tanh(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8940f3a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lenet(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv3): Conv2d(16, 120, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc2): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Lenet()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02647ec6",
   "metadata": {},
   "source": [
    "```\n",
    "uv add torch-summary\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bcd90f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5b42b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Conv2d: 1-1                            156\n",
      "├─Conv2d: 1-2                            2,416\n",
      "├─Conv2d: 1-3                            48,120\n",
      "├─Linear: 1-4                            10,164\n",
      "├─Linear: 1-5                            850\n",
      "=================================================================\n",
      "Total params: 61,706\n",
      "Trainable params: 61,706\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "├─Conv2d: 1-1                            156\n",
       "├─Conv2d: 1-2                            2,416\n",
       "├─Conv2d: 1-3                            48,120\n",
       "├─Linear: 1-4                            10,164\n",
       "├─Linear: 1-5                            850\n",
       "=================================================================\n",
       "Total params: 61,706\n",
       "Trainable params: 61,706\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size=(1, 32, 32), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79db290a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loss ::: 0.806262731552124\n",
      "2 loss ::: 0.8068898916244507\n",
      "3 loss ::: 0.797527551651001\n",
      "4 loss ::: 0.8375831842422485\n",
      "5 loss ::: 0.7971524596214294\n",
      "6 loss ::: 0.8716562986373901\n",
      "7 loss ::: 0.7989715337753296\n",
      "8 loss ::: 0.8001868724822998\n",
      "9 loss ::: 0.7987685203552246\n",
      "10 loss ::: 0.8456776142120361\n"
     ]
    }
   ],
   "source": [
    "# 학습시키기\n",
    "lr = 1e-3\n",
    "optim = Adam(model.parameters(), lr=lr)\n",
    "epochs = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "step = 0\n",
    "\n",
    "for epoch in range(epochs) :\n",
    "    for data, label in train_loader :\n",
    "        optim.zero_grad()\n",
    "        pred = model(data.to(device))  # [32, 1, 32, 32]\n",
    "        loss = criterion(pred, label.to(device))\n",
    "        writer.add_scalar(\"Loss/train\", loss, step)\n",
    "        step += 1\n",
    "\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "    print(f\"{epoch+1} loss ::: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073e6de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 모델 평가 코드작성 (정확도)\n",
    "# 2. 이미지파일(data/4.jpg)을 불러와서 모델에 넣어 결과 받기\n",
    "# 2-1. 이미지 불러오기\n",
    "# 2-2. 이미지를 학습할때와 같이 전처리 해준다\n",
    "# 2-3. 모델에 넣고 추론\n",
    "# 결과 : [-, -, -, -, @, -, -, ...] -> 4 값이 제일 높게 나와야 정답\n",
    "# 모델에 이미지 넣을때 주의할 점 : shape 똑같게 해야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be342b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 : 0.9883\n"
     ]
    }
   ],
   "source": [
    "# 평가\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad() :\n",
    "    total_corr = 0\n",
    "    for images, labels in test_loader :\n",
    "        # images = images.view(-1, 120).to(device)\n",
    "        \n",
    "        labels = labels.to(device)\n",
    "\n",
    "        preds = model(images.to(device))\n",
    "\n",
    "        _, pred = torch.max(preds.data, 1)\n",
    "        total_corr = total_corr + (pred == labels).sum().item()\n",
    "\n",
    "print(f\"정확도 : {total_corr/len(test_data.targets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2ecd407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 파일 불러오기\n",
    "from PIL import Image\n",
    "\n",
    "img = Image.open(\"data/4.jpg\").convert(\"L\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40ceb95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 전처리\n",
    "img_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),              # tensor 변환\n",
    "        transforms.Resize(32),              # 이미지 크기변경\n",
    "        transforms.Normalize((0.5), (1.0))  # 정규화 (평균, 표준편차)\n",
    "    ]\n",
    ")\n",
    "input_tensor = img_transform(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "35fa1206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_tensor의 사이즈 확인\n",
    "input_tensor.size()\n",
    "\n",
    "# model : [32, 1, 32, 32]\n",
    "# unsqueeze 사용\n",
    "input_tensor = input_tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc69898e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 불러오기 & 추론\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19c0b620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예측결과 해석\n",
    "predict = output.argmax(dim=1).item()\n",
    "predict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
